#!/usr/bin/env python3
"""
IRIS+ ë©”íŠ¸ë¦­ ìƒì„¸ í˜ì´ì§€ ë¶„ì„ ë„êµ¬
ë§ˆì§€ë§‰ 30ê°œ ë©”íŠ¸ë¦­ì˜ detail_urlì—ì„œ .content-area ë‚´ìš©ì„ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import json
import requests
from bs4 import BeautifulSoup
import time
import logging
from typing import List, Dict, Optional

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data_temp/detail_analysis.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DetailAnalyzer:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
    def load_metrics_data(self, filename: str = "data/iris_metrics.json") -> List[Dict]:
        """JSON íŒŒì¼ì—ì„œ ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data['metrics']
        except Exception as e:
            logger.error(f"JSON íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def get_page_content(self, url: str) -> Optional[BeautifulSoup]:
        """ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except requests.RequestException as e:
            logger.error(f"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ {url}: {e}")
            return None
    
    def analyze_content_area(self, soup: BeautifulSoup, metric_info: Dict) -> Dict:
        """content-area ë‚´ìš©ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        analysis = {
            'title': metric_info['title'],
            'data_id': metric_info['data_id'],
            'url': metric_info['detail_url'],
            'content_found': False,
            'elements': {},
            'text_content': "",
            'error': None
        }
        
        try:
            # .content-area ìš”ì†Œ ì°¾ê¸°
            content_area = soup.find(class_='content-area')
            if not content_area:
                analysis['error'] = "content-area í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"
                return analysis
            
            analysis['content_found'] = True
            
            # ë‹¤ì–‘í•œ ìš”ì†Œë“¤ ë¶„ì„
            elements = analysis['elements']
            
            # ì œëª©ë“¤ (h1, h2, h3 ë“±)
            headings = content_area.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            if headings:
                elements['headings'] = [{'tag': h.name, 'text': h.get_text(strip=True)} for h in headings]
            
            # ë‹¨ë½ë“¤
            paragraphs = content_area.find_all('p')
            if paragraphs:
                elements['paragraphs'] = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
            
            # ë¦¬ìŠ¤íŠ¸ë“¤
            lists = content_area.find_all(['ul', 'ol'])
            if lists:
                elements['lists'] = []
                for lst in lists:
                    items = [li.get_text(strip=True) for li in lst.find_all('li')]
                    elements['lists'].append({'type': lst.name, 'items': items})
            
            # í…Œì´ë¸”ë“¤
            tables = content_area.find_all('table')
            if tables:
                elements['tables'] = len(tables)
            
            # ë§í¬ë“¤
            links = content_area.find_all('a', href=True)
            if links:
                elements['links'] = [{'text': a.get_text(strip=True), 'href': a['href']} for a in links]
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ë‚´ìš© (ìš”ì•½ìš©)
            analysis['text_content'] = content_area.get_text(separator=' ', strip=True)[:500] + "..."
            
            # íŠ¹ë³„í•œ í´ë˜ìŠ¤ë“¤ ì°¾ê¸°
            special_elements = content_area.find_all(class_=True)
            unique_classes = set()
            for elem in special_elements:
                if elem.get('class'):
                    unique_classes.update(elem['class'])
            
            if unique_classes:
                elements['css_classes'] = sorted(list(unique_classes))
            
        except Exception as e:
            analysis['error'] = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        
        return analysis
    
    def analyze_last_n_metrics(self, n: int = 30) -> List[Dict]:
        """ë§ˆì§€ë§‰ nê°œ ë©”íŠ¸ë¦­ì˜ ìƒì„¸ í˜ì´ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        logger.info(f"ë§ˆì§€ë§‰ {n}ê°œ ë©”íŠ¸ë¦­ ìƒì„¸ í˜ì´ì§€ ë¶„ì„ ì‹œì‘...")
        
        # ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ
        metrics = self.load_metrics_data()
        if not metrics:
            logger.error("ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ë§ˆì§€ë§‰ nê°œ ì„ íƒ
        last_metrics = metrics[-n:]
        logger.info(f"ë¶„ì„í•  ë©”íŠ¸ë¦­: {len(last_metrics)}ê°œ")
        
        results = []
        
        for i, metric in enumerate(last_metrics, 1):
            logger.info(f"[{i}/{len(last_metrics)}] ë¶„ì„ ì¤‘: {metric['title']} ({metric['data_id']})")
            
            # ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            soup = self.get_page_content(metric['detail_url'])
            if soup is None:
                logger.warning(f"í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {metric['detail_url']}")
                continue
            
            # content-area ë¶„ì„
            analysis = self.analyze_content_area(soup, metric)
            results.append(analysis)
            
            logger.info(f"ë¶„ì„ ì™„ë£Œ: {'ì„±ê³µ' if analysis['content_found'] else 'ì‹¤íŒ¨'}")
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€
            time.sleep(1)
        
        logger.info(f"ì „ì²´ ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        return results
    
    def save_analysis_results(self, results: List[Dict], filename: str = "data_temp/detail_analysis.json"):
        """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            output_data = {
                "metadata": {
                    "total_analyzed": len(results),
                    "analyzed_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "successful_analyses": len([r for r in results if r['content_found']])
                },
                "analyses": results
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ë¶„ì„ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return filename
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def print_summary(self, results: List[Dict]):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print("\n" + "="*80)
        print("ğŸ“Š CONTENT-AREA ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        successful = [r for r in results if r['content_found']]
        failed = [r for r in results if not r['content_found']]
        
        print(f"âœ… ì„±ê³µ: {len(successful)}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {len(failed)}ê°œ")
        print(f"ğŸ“ˆ ì„±ê³µë¥ : {len(successful)/len(results)*100:.1f}%")
        
        if successful:
            print(f"\nğŸ“‹ ë°œê²¬ëœ ê³µí†µ ìš”ì†Œë“¤:")
            
            # ê³µí†µ CSS í´ë˜ìŠ¤ ë¶„ì„
            all_classes = set()
            for result in successful:
                if 'css_classes' in result['elements']:
                    all_classes.update(result['elements']['css_classes'])
            
            if all_classes:
                print(f"ğŸ¨ CSS í´ë˜ìŠ¤ë“¤: {', '.join(sorted(list(all_classes))[:10])}...")
            
            # í‰ê·  ìš”ì†Œ ê°œìˆ˜
            avg_paragraphs = sum(len(r['elements'].get('paragraphs', [])) for r in successful) / len(successful)
            avg_headings = sum(len(r['elements'].get('headings', [])) for r in successful) / len(successful)
            
            print(f"ğŸ“ í‰ê·  ë‹¨ë½ ìˆ˜: {avg_paragraphs:.1f}ê°œ")
            print(f"ğŸ“– í‰ê·  ì œëª© ìˆ˜: {avg_headings:.1f}ê°œ")
        
        print("\n" + "="*80)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = DetailAnalyzer()
    
    # ë§ˆì§€ë§‰ 30ê°œ ë©”íŠ¸ë¦­ ë¶„ì„
    results = analyzer.analyze_last_n_metrics(30)
    
    if results:
        # ê²°ê³¼ ì €ì¥
        filename = analyzer.save_analysis_results(results)
        
        # ìš”ì•½ ì¶œë ¥
        analyzer.print_summary(results)
        
        # ëª‡ ê°€ì§€ ìƒ˜í”Œ ì¶œë ¥
        print("\nğŸ” ìƒ˜í”Œ ë¶„ì„ ê²°ê³¼:")
        for i, result in enumerate(results[:3]):  # ì²˜ìŒ 3ê°œë§Œ
            print(f"\n{i+1}. {result['title']} ({result['data_id']})")
            if result['content_found']:
                elements = result['elements']
                if 'headings' in elements:
                    print(f"   ì œëª©ë“¤: {[h['text'] for h in elements['headings'][:3]]}")
                if 'paragraphs' in elements:
                    print(f"   ë‹¨ë½ ìˆ˜: {len(elements['paragraphs'])}ê°œ")
                print(f"   í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {result['text_content'][:100]}...")
            else:
                print(f"   ì˜¤ë¥˜: {result['error']}")
        
        if filename:
            print(f"\nğŸ“ ì „ì²´ ê²°ê³¼ëŠ” {filename}ì—ì„œ í™•ì¸í•˜ì„¸ìš”!")
    else:
        print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
