#!/usr/bin/env python3
"""
metric-box êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ìµœì¢… IRIS+ ë©”íŠ¸ë¦­ ìŠ¤í¬ë˜í¼
"""

import json
import requests
from bs4 import BeautifulSoup
import time
import logging
from typing import List, Dict, Optional
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data_temp/final_scraping.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FinalScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
    def load_base_metrics(self, filename: str = "data/iris_metrics.json") -> Dict:
        """ê¸°ì¡´ ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"ê¸°ì¡´ JSON íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def get_page_content(self, url: str) -> Optional[BeautifulSoup]:
        """ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except requests.RequestException as e:
            logger.error(f"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ {url}: {e}")
            return None
    
    def extract_section_content(self, section) -> Dict:
        """section íƒœê·¸ì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        content = {
            'paragraphs': [],
            'lists': [],
            'headings': [],
            'other_elements': [],
            'raw_text': ''
        }
        
        try:
            # ëª¨ë“  ì œëª© ìš”ì†Œ ì¶”ì¶œ (h1-h6)
            headings = section.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            for h in headings:
                text = h.get_text(strip=True)
                if text:
                    content['headings'].append({
                        'tag': h.name,
                        'text': text
                    })
            
            # ëª¨ë“  ë‹¨ë½ ì¶”ì¶œ
            paragraphs = section.find_all('p')
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text:
                    content['paragraphs'].append(text)
            
            # ëª¨ë“  ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            lists = section.find_all(['ul', 'ol'])
            for lst in lists:
                list_items = []
                for li in lst.find_all('li'):
                    item_text = li.get_text(strip=True)
                    if item_text:
                        list_items.append(item_text)
                
                if list_items:
                    content['lists'].append({
                        'type': lst.name,
                        'items': list_items
                    })
            
            # ê¸°íƒ€ ì¤‘ìš”í•œ ìš”ì†Œë“¤ (div, span ë“±ì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸)
            other_elements = section.find_all(['div', 'span'])
            for elem in other_elements:
                # ìì‹ ìš”ì†Œê°€ ì—†ê³  ì§ì ‘ í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°
                if not elem.find_all() and elem.get_text(strip=True):
                    text = elem.get_text(strip=True)
                    # ì´ë¯¸ ë‹¤ë¥¸ ê³³ì—ì„œ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ë§Œ ì¶”ê°€
                    if text not in content['raw_text']:
                        content['other_elements'].append({
                            'tag': elem.name,
                            'text': text,
                            'class': elem.get('class', [])
                        })
            
            # ì „ì²´ í…ìŠ¤íŠ¸ (ë°±ì—…ìš©)
            content['raw_text'] = section.get_text(separator=' ', strip=True)
            
        except Exception as e:
            logger.error(f"ì„¹ì…˜ ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return content
    
    def extract_metadata(self, soup: BeautifulSoup) -> Dict:
        """section#metadataì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        metadata = {}
        
        try:
            # section#metadata ì°¾ê¸°
            metadata_section = soup.find('section', id='metadata')
            if metadata_section:
                # ul íƒœê·¸ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata_list = metadata_section.find('ul')
                if metadata_list:
                    for li in metadata_list.find_all('li'):
                        text = li.get_text(strip=True)
                        if 'Reporting Format' in text:
                            metadata['Reporting Format'] = text.replace('Reporting Format', '').strip()
                        elif 'Metric Type' in text:
                            metadata['Metric Type'] = text.replace('Metric Type', '').strip()
                        elif 'Metric Level' in text:
                            metadata['Metric Level'] = text.replace('Metric Level', '').strip()
                        elif 'IRIS Metric Citation' in text:
                            metadata['IRIS Metric Citation'] = text.replace('IRIS Metric Citation', '').strip()
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return metadata

    def extract_metric_details(self, soup: BeautifulSoup) -> Dict:
        """metric-box êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        content_area = soup.find(class_='content-area')
        if not content_area:
            return {}
        
        details = {}
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        details['metadata'] = self.extract_metadata(soup)
        
        # ëª¨ë“  metric-box ì°¾ê¸°
        metric_boxes = content_area.find_all('div', class_='metric-box')
        
        for box in metric_boxes:
            try:
                # headerì—ì„œ ì œëª© ì¶”ì¶œ
                header = box.find(['header', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                if not header:
                    continue
                
                section_title = header.get_text(strip=True)
                
                # sectionì—ì„œ ë‚´ìš© ì¶”ì¶œ
                section = box.find('section')
                if section:
                    section_content = self.extract_section_content(section)
                else:
                    # sectionì´ ì—†ìœ¼ë©´ boxì—ì„œ ì§ì ‘ ì¶”ì¶œ
                    section_content = self.extract_section_content(box)
                
                # ì„¹ì…˜ë³„ë¡œ ì ì ˆí•œ í‚¤ ìƒì„±
                if 'Account Value' in section_title or any(id_pattern in section_title for id_pattern in ['(PI', '(FP', '(OI', '(PD', '(OD']):
                    # ë©”íŠ¸ë¦­ ì •ì˜ ì„¹ì…˜
                    details['definition'] = {
                        'title': section_title,
                        'content': section_content
                    }
                elif 'Usage Guidance' in section_title:
                    details['usage_guidance'] = {
                        'title': section_title,
                        'content': section_content
                    }
                elif 'Impact Categories' in section_title:
                    details['impact_categories'] = {
                        'title': section_title,
                        'content': section_content
                    }
                elif 'SDG Goals' in section_title:
                    details['sdg_goals'] = {
                        'title': section_title,
                        'content': section_content
                    }
                elif 'Metric History' in section_title:
                    details['metric_history'] = {
                        'title': section_title,
                        'content': section_content
                    }
                elif 'Related metrics' in section_title:
                    details['related_metrics'] = {
                        'title': section_title,
                        'content': section_content
                    }
                else:
                    # ê¸°íƒ€ ì„¹ì…˜ë“¤
                    key = section_title.lower().replace(' ', '_').replace('&', 'and')
                    details[key] = {
                        'title': section_title,
                        'content': section_content
                    }
                
            except Exception as e:
                logger.error(f"metric-box ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        return details
    
    def process_single_metric(self, metric: Dict) -> Dict:
        """ë‹¨ì¼ ë©”íŠ¸ë¦­ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            # ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            soup = self.get_page_content(metric['detail_url'])
            if soup is None:
                return {
                    **metric,
                    'details': {
                        'success': False,
                        'error': 'Page load failed',
                        'scraped_at': datetime.now().isoformat()
                    }
                }
            
            # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            details = self.extract_metric_details(soup)
            details['success'] = True
            details['scraped_at'] = datetime.now().isoformat()
            
            return {
                **metric,
                'details': details
            }
            
        except Exception as e:
            logger.error(f"ë©”íŠ¸ë¦­ {metric['data_id']} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                **metric,
                'details': {
                    'success': False,
                    'error': str(e),
                    'scraped_at': datetime.now().isoformat()
                }
            }
    
    def process_all_metrics(self, base_data: Dict, batch_size: int = 50) -> Dict:
        """ëª¨ë“  ë©”íŠ¸ë¦­ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        metrics = base_data['metrics']
        total_metrics = len(metrics)
        
        logger.info(f"ì „ì²´ {total_metrics}ê°œ ë©”íŠ¸ë¦­ ì²˜ë¦¬ ì‹œì‘")
        
        for start_idx in range(0, total_metrics, batch_size):
            end_idx = min(start_idx + batch_size, total_metrics)
            batch_num = start_idx // batch_size + 1
            total_batches = (total_metrics + batch_size - 1) // batch_size
            
            logger.info(f"ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘ ({start_idx+1}-{end_idx})")
            
            for i in range(start_idx, end_idx):
                current_progress = i - start_idx + 1
                batch_total = end_idx - start_idx
                
                logger.info(f"[{current_progress}/{batch_total}] {metrics[i]['title']} ({metrics[i]['data_id']})")
                
                # ë©”íŠ¸ë¦­ ì²˜ë¦¬
                metrics[i] = self.process_single_metric(metrics[i])
                
                success = metrics[i]['details']['success']
                logger.info(f"ì²˜ë¦¬ {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€
                time.sleep(1.5)
            
            # ì¤‘ê°„ ì €ì¥
            temp_filename = f"data_temp/final_metrics_temp_{batch_num}.json"
            with open(temp_filename, 'w', encoding='utf-8') as f:
                json.dump(base_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ë°°ì¹˜ {batch_num} ì™„ë£Œ - ì¤‘ê°„ ì €ì¥ë¨")
            
            # ë°°ì¹˜ ê°„ íœ´ì‹
            if end_idx < total_metrics:
                logger.info("ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•´ 3ì´ˆ ëŒ€ê¸°...")
                time.sleep(3)
        
        return base_data

def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜ - ì²˜ìŒ 5ê°œë§Œ ì²˜ë¦¬"""
    scraper = FinalScraper()
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    logger.info("ê¸°ì¡´ ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ ì¤‘...")
    base_data = scraper.load_base_metrics()
    
    if not base_data:
        print("âŒ ê¸°ì¡´ ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ§ª í…ŒìŠ¤íŠ¸: ì²˜ìŒ 5ê°œ ë©”íŠ¸ë¦­ ì²˜ë¦¬")
    
    # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
    test_metrics = base_data['metrics'][:5]
    for i, metric in enumerate(test_metrics):
        logger.info(f"[{i+1}/5] {metric['title']} ({metric['data_id']})")
        test_metrics[i] = scraper.process_single_metric(metric)
        success = test_metrics[i]['details']['success']
        logger.info(f"ì²˜ë¦¬ {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
        time.sleep(1)
    
    # ê²°ê³¼ ì €ì¥
    base_data['metrics'] = test_metrics + base_data['metrics'][5:]
    with open('data_temp/final_test_metrics.json', 'w', encoding='utf-8') as f:
        json.dump(base_data, f, ensure_ascii=False, indent=2)
    
    # ê²°ê³¼ í™•ì¸
    successful = sum(1 for m in test_metrics if m['details']['success'])
    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {successful}/5ê°œ ì„±ê³µ")
    
    # ìƒ˜í”Œ ì¶œë ¥
    if successful > 0:
        sample = test_metrics[0]
        if sample['details']['success']:
            print(f"\nğŸ“‹ ìƒ˜í”Œ: {sample['title']}")
            
            details = sample['details']
            for key, value in details.items():
                if key in ['success', 'scraped_at', 'error']:
                    continue
                if isinstance(value, dict) and 'content' in value:
                    content = value['content']
                    para_count = len(content.get('paragraphs', []))
                    list_count = len(content.get('lists', []))
                    print(f"  {value['title']}: ë‹¨ë½ {para_count}ê°œ, ë¦¬ìŠ¤íŠ¸ {list_count}ê°œ")

if __name__ == "__main__":
    main()
