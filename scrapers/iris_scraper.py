#!/usr/bin/env python3
"""
IRIS+ ë©”íŠ¸ë¦­ ë°ì´í„° ìŠ¤í¬ë˜í•‘ ë„êµ¬
https://iris.thegiin.org/metrics/ ì—ì„œ ë©”íŠ¸ë¦­ ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import logging
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data_temp/iris_scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class IRISScraper:
    def __init__(self, base_url: str = "https://iris.thegiin.org"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
    def get_page_content(self, url: str) -> Optional[BeautifulSoup]:
        """ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except requests.RequestException as e:
            logger.error(f"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ {url}: {e}")
            return None
    
    def extract_metrics_from_page(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """í˜ì´ì§€ì—ì„œ ë©”íŠ¸ë¦­ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        metrics = []
        
        # .catalog-list ìš”ì†Œ ì°¾ê¸°
        catalog_list = soup.find(class_='catalog-list')
        if not catalog_list:
            logger.warning("catalog-list ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return metrics
        
        # a íƒœê·¸ë“¤ ì°¾ê¸°
        metric_links = catalog_list.find_all('a', href=True)
        
        for link in metric_links:
            try:
                # href ì†ì„±ì—ì„œ ê²½ë¡œ ì¶”ì¶œ
                href = link.get('href', '')
                
                # span.id ìš”ì†Œì—ì„œ data-id ì¶”ì¶œ
                span_id = link.find('span', class_='id')
                data_id = span_id.get('data-id', '') if span_id else ''
                
                # ë§í¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ (span íƒœê·¸ ë‚´ìš© ì œì™¸)
                text_content = link.get_text(strip=True)
                # (PI1653) ê°™ì€ ë¶€ë¶„ ì œê±°
                if '(' in text_content and ')' in text_content:
                    text_content = text_content.split('(')[0].strip()
                
                metric_info = {
                    'title': text_content,
                    'data_id': data_id,
                    'relative_path': href,
                    'detail_url': f"{self.base_url}/metric/5.3b/{data_id.lower()}/" if data_id else ""
                }
                
                metrics.append(metric_info)
                logger.debug(f"ë©”íŠ¸ë¦­ ì¶”ì¶œ: {metric_info['title']} ({data_id})")
                
            except Exception as e:
                logger.error(f"ë©”íŠ¸ë¦­ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        return metrics
    
    def scrape_all_pages(self, total_pages: int = 63) -> List[Dict[str, str]]:
        """ëª¨ë“  í˜ì´ì§€ì—ì„œ ë©”íŠ¸ë¦­ì„ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤."""
        all_metrics = []
        
        logger.info(f"ì´ {total_pages}í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
        
        for page_num in range(1, total_pages + 1):
            url = f"{self.base_url}/metrics/?page={page_num}"
            logger.info(f"í˜ì´ì§€ {page_num}/{total_pages} ì²˜ë¦¬ ì¤‘: {url}")
            
            soup = self.get_page_content(url)
            if soup is None:
                logger.error(f"í˜ì´ì§€ {page_num} ìŠ¤í‚µ")
                continue
            
            page_metrics = self.extract_metrics_from_page(soup)
            all_metrics.extend(page_metrics)
            
            logger.info(f"í˜ì´ì§€ {page_num}ì—ì„œ {len(page_metrics)}ê°œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘")
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
            time.sleep(1)
        
        logger.info(f"ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: ì´ {len(all_metrics)}ê°œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘")
        return all_metrics
    
    def save_to_json(self, metrics: List[Dict[str, str]], filename: str = "data/iris_metrics.json"):
        """ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            # ë©”íƒ€ë°ì´í„° í¬í•¨í•œ ìµœì¢… êµ¬ì¡°
            output_data = {
                "metadata": {
                    "source": "https://iris.thegiin.org/metrics/",
                    "total_metrics": len(metrics),
                    "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "version": "IRIS v5.3b"
                },
                "metrics": metrics
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return filename
            
        except Exception as e:
            logger.error(f"JSON ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = IRISScraper()
    
    # ëª¨ë“  í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
    metrics = scraper.scrape_all_pages(total_pages=63)
    
    if metrics:
        # JSON íŒŒì¼ë¡œ ì €ì¥
        filename = scraper.save_to_json(metrics)
        
        if filename:
            print(f"\nâœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
            print(f"ğŸ“ íŒŒì¼: {filename}")
            print(f"ğŸ“Š ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­: {len(metrics)}ê°œ")
            
            # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
            if len(metrics) > 0:
                print(f"\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„°:")
                for i, metric in enumerate(metrics[:3]):
                    print(f"{i+1}. {metric['title']} ({metric['data_id']})")
                    print(f"   ìƒì„¸ URL: {metric['detail_url']}")
        else:
            print("âŒ JSON ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
