"""
ë°ì´í„° ê´€ë¦¬ í†µí•© ìŠ¤í¬ë¦½íŠ¸
"""
import argparse
import json
from pathlib import Path
from datetime import datetime
from data_catalog import DataCatalog, DataFile
from config.data_sources_config import get_data_source_config

def register_existing_files():
    """ê¸°ì¡´ íŒŒì¼ë“¤ì„ ì¹´íƒˆë¡œê·¸ì— ë“±ë¡"""
    catalog = DataCatalog()
    
    # IRIS ë°ì´í„° íŒŒì¼ë“¤ ë“±ë¡
    iris_files = [
        {
            "filename": "iris_metrics_v1_20241219.json",
            "data_source": "iris",
            "data_type": "metrics",
            "version": "v1",
            "file_path": "data_sources/iris/raw/iris_metrics_v1_20241219.json",
            "description": "IRIS+ ë©”íŠ¸ë¦­ ê¸°ë³¸ ëª©ë¡ (750ê°œ)",
            "tags": ["iris", "metrics", "impact_investing"]
        },
        {
            "filename": "iris_metrics_complete_v1_20241219.json",
            "data_source": "iris",
            "data_type": "metrics",
            "version": "v1",
            "file_path": "data_sources/iris/processed/iris_metrics_complete_v1_20241219.json",
            "description": "IRIS+ ë©”íŠ¸ë¦­ ìƒì„¸ ì •ë³´ í¬í•¨ ì™„ì „ ë°ì´í„°",
            "tags": ["iris", "metrics", "detailed", "impact_investing"]
        },
        {
            "filename": "iris_metrics_supabase_v1_20241219.json",
            "data_source": "iris",
            "data_type": "metrics",
            "version": "v1",
            "file_path": "data_sources/iris/processed/iris_metrics_supabase_v1_20241219.json",
            "description": "Supabase í˜•ì‹ìœ¼ë¡œ ë³€í™˜ëœ IRIS+ ë©”íŠ¸ë¦­ ë°ì´í„°",
            "tags": ["iris", "metrics", "supabase", "database"]
        }
    ]
    
    for file_info in iris_files:
        file_path = Path(file_info["file_path"])
        if file_path.exists():
            file_size = file_path.stat().st_size
            data_file = DataFile(
                filename=file_info["filename"],
                data_source=file_info["data_source"],
                data_type=file_info["data_type"],
                version=file_info["version"],
                file_path=file_info["file_path"],
                file_size=file_size,
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat(),
                description=file_info["description"],
                tags=file_info["tags"]
            )
            catalog.register_data_file(data_file)
        else:
            print(f"âš ï¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_info['file_path']}")

def list_data_sources():
    """ë°ì´í„° ì†ŒìŠ¤ ëª©ë¡ í‘œì‹œ"""
    catalog = DataCatalog()
    sources = catalog.list_data_sources()
    
    print("ğŸ“Š ë“±ë¡ëœ ë°ì´í„° ì†ŒìŠ¤:")
    for source in sources:
        print(f"  â€¢ {source.code}: {source.name}")
        print(f"    - ì„¤ëª…: {source.description}")
        print(f"    - ì›¹ì‚¬ì´íŠ¸: {source.website}")
        print(f"    - ì—…ë°ì´íŠ¸ ì£¼ê¸°: {source.update_frequency}")
        print()

def list_data_files(data_source=None):
    """ë°ì´í„° íŒŒì¼ ëª©ë¡ í‘œì‹œ"""
    catalog = DataCatalog()
    files = catalog.list_data_files(data_source)
    
    if data_source:
        print(f"ğŸ“ {data_source} ë°ì´í„° íŒŒì¼:")
    else:
        print("ğŸ“ ëª¨ë“  ë°ì´í„° íŒŒì¼:")
    
    for file in files:
        size_mb = file.file_size / (1024 * 1024)
        print(f"  â€¢ {file.filename}")
        print(f"    - ê²½ë¡œ: {file.file_path}")
        print(f"    - í¬ê¸°: {size_mb:.2f}MB")
        print(f"    - ì„¤ëª…: {file.description}")
        print(f"    - íƒœê·¸: {', '.join(file.tags)}")
        print()

def show_stats():
    """ë°ì´í„° í†µê³„ í‘œì‹œ"""
    catalog = DataCatalog()
    stats = catalog.get_file_stats()
    
    print("ğŸ“Š ë°ì´í„° í†µê³„:")
    print(f"  â€¢ ì´ íŒŒì¼ ìˆ˜: {stats['total_files']}ê°œ")
    print(f"  â€¢ ì´ ìš©ëŸ‰: {stats['total_size_mb']}MB")
    print()
    
    print("ğŸ“ ë°ì´í„° ì†ŒìŠ¤ë³„ íŒŒì¼ ìˆ˜:")
    for source, count in stats['by_source'].items():
        print(f"  â€¢ {source}: {count}ê°œ")
    print()
    
    print("ğŸ“„ ë°ì´í„° ìœ í˜•ë³„ íŒŒì¼ ìˆ˜:")
    for data_type, count in stats['by_type'].items():
        print(f"  â€¢ {data_type}: {count}ê°œ")

def search_files(query):
    """íŒŒì¼ ê²€ìƒ‰"""
    catalog = DataCatalog()
    results = catalog.search_files(query)
    
    print(f"ğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):")
    for file in results:
        print(f"  â€¢ {file.filename} ({file.data_source})")
        print(f"    - {file.description}")

def generate_report():
    """ë¦¬í¬íŠ¸ ìƒì„±"""
    catalog = DataCatalog()
    report = catalog.generate_report()
    
    report_file = Path("data_catalog_report.md")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“„ ë¦¬í¬íŠ¸ ìƒì„±ë¨: {report_file}")
    print(report)

def main():
    parser = argparse.ArgumentParser(description='ë°ì´í„° ê´€ë¦¬ ë„êµ¬')
    parser.add_argument('--register', action='store_true', help='ê¸°ì¡´ íŒŒì¼ë“¤ ë“±ë¡')
    parser.add_argument('--list-sources', action='store_true', help='ë°ì´í„° ì†ŒìŠ¤ ëª©ë¡')
    parser.add_argument('--list-files', help='ë°ì´í„° íŒŒì¼ ëª©ë¡ (ì†ŒìŠ¤ ì§€ì • ê°€ëŠ¥)')
    parser.add_argument('--stats', action='store_true', help='ë°ì´í„° í†µê³„')
    parser.add_argument('--search', help='íŒŒì¼ ê²€ìƒ‰')
    parser.add_argument('--report', action='store_true', help='ë¦¬í¬íŠ¸ ìƒì„±')
    
    args = parser.parse_args()
    
    if args.register:
        register_existing_files()
    elif args.list_sources:
        list_data_sources()
    elif args.list_files is not None:
        list_data_files(args.list_files if args.list_files else None)
    elif args.stats:
        show_stats()
    elif args.search:
        search_files(args.search)
    elif args.report:
        generate_report()
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
